+++
title = "Provably learning a multi-head attention layer"
author = "Sitan Chen (Harvard, Cambridge MA)"
author_link = "https://sitanchen.com/"
author_image = "assets/authorImages/SitanChen.jpg"
date = "2024-02-15T18:30:00+05:30"
date_end = "2024-02-15T19:30:00+05:30"
location = "Online Talk on Microsoft Teams"
location_link = "https://teams.microsoft.com/l/meetup-join/19%3ameeting_ZGE3NDg5NzktMWQ0Zi00MzFmLTg5OTgtMTMyYWM4MWQyYjI2%40thread.v2/0?context=%7b%22Tid%22%3a%226f15cd97-f6a7-41e3-b2c5-ad4193976476%22%2c%22Oid%22%3a%227c84465e-c38b-4d7a-9a9d-ff0dfa3638b3%22%7d"
notes = "Jointly organized
by <a href = "https://www.microsoft.com/en-us/research/lab/microsoft-research-india/" target= "_blank">Microsoft
Research Lab - India</a> and <a href='https://www.csa.iisc.ac.in/theoretical-computer-science/' target= "_blank">
Theoretical Computer Science @ IISc</a>"
+++

<b>Abstract:</b>
Despite the widespread empirical success of transformers, little is known about their learnability from a computational 
perspective. In practice these models are trained with SGD on a certain next-token prediction objective, but in theory 
it remains a mystery even to prove that such functions can be learned efficiently at all. In this work, we give the 
first nontrivial provable algorithms and computational lower bounds for this problem. Our results apply in a realizable 
setting where one is given random sequence-to-sequence pairs that are generated by some unknown multi-head attention 
layer. Our algorithm, which is centered around using examples to sculpt a convex body containing the unknown parameters, 
is a significant departure from existing provable algorithms for learning multi-layer perceptrons, which predominantly 
exploit fine-grained algebraic and rotation invariance properties of the input distribution. 
<br><br>
Joint work with Yuanzhi Li.
