+++
title = "Provable Size Requirements for Operator Learning and PINNs"
author = "Anirbit Mukherjee (University of Manchester)"
author_link = "https://anirbit-ai.github.io"
author_image = "assets/authorImages/AnirbitMukherjee.jpeg"
date = "2025-08-27T16:00:00+05:30"
date_end = "2025-08-27T17:00:00+05:30"
location = "YouTube Video Link"
location_link = "https://youtu.be/CWvnhv1nMRY"
notes = "We are grateful to the <a href = "https://www.accel.com/people/shekhar-kirani" target= "_blank">Shekhar Kirani</a> family and the <a href = "https://www.csa.iisc.ac.in/cfe-walmart/" target= "_blank">Walmart Center for Tech Excellence</a> for generously supporting this seminar series."
+++

<b>Abstract:</b>
An ongoing revolution in machine learning is about being able to set up neural systems that can approximate maps between Banach spaces. The most basic such setup is that of Deep Operator Nets (DeepONets). We will begin this talk by introducing this fascinating idea and how it leads to a mechanism of using machine learning to solve systems of Partial Differential Equations. Next, we will focus on our recent work proving “universal”/data-independent size requirements for DeepONets, for them to be able to perform well. We will emphasize the modularity of the proof structure and why it's readily adaptable to many other ML systems. Thus, we pave the path towards a plethora of research avenues for deriving model size requirements for many other ML scenarios.
<br><br>
This talk is largely based on our work with Amartya Roy (now a PhD student at IIT-Delhi), published in Transactions in Machine Learning (TMLR) in 2024, <a href="https://openreview.net/pdf?id=RwmWODTNFE" target="_blank">https://openreview.net/pdf?id=RwmWODTNFE</a>. Towards the end, we will touch upon the following work, <a href="https://arxiv.org/abs/2507.06967" target="_blank">https://arxiv.org/abs/2507.06967</a>, where a similar result was obtained for PINNs, with my PhD student Sebastien Andre-Sloan and Prof. Matthew Colbrook (DAMTP, Cambridge).